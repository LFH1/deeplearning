{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nnet = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n\nX = torch.rand(2, 20)\nnet(X)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:27:21.474295Z","iopub.execute_input":"2024-06-24T03:27:21.475011Z","iopub.status.idle":"2024-06-24T03:27:24.905372Z","shell.execute_reply.started":"2024-06-24T03:27:21.474975Z","shell.execute_reply":"2024-06-24T03:27:24.904185Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.0623, -0.2223, -0.0473, -0.2838, -0.0430,  0.2824, -0.0648, -0.1781,\n         -0.1555, -0.0877],\n        [ 0.0601, -0.1904, -0.0054, -0.3576, -0.1004,  0.2816, -0.1508, -0.1698,\n         -0.0573, -0.1281]], grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"class MLP(nn.Module):\n    # 用模型参数声明层。这里，我们声明两个全连接的层\n    def __init__(self):\n        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n        super().__init__()\n        self.hidden = nn.Linear(20, 256)  # 隐藏层\n        self.out = nn.Linear(256, 10)  # 输出层\n\n    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n    def forward(self, X):\n        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n        return self.out(F.relu(self.hidden(X)))","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:33:48.821484Z","iopub.execute_input":"2024-06-24T03:33:48.822032Z","iopub.status.idle":"2024-06-24T03:33:48.828784Z","shell.execute_reply.started":"2024-06-24T03:33:48.821998Z","shell.execute_reply":"2024-06-24T03:33:48.827593Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"net = MLP()\nnet(X)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:35:04.479767Z","iopub.execute_input":"2024-06-24T03:35:04.480139Z","iopub.status.idle":"2024-06-24T03:35:04.489308Z","shell.execute_reply.started":"2024-06-24T03:35:04.480108Z","shell.execute_reply":"2024-06-24T03:35:04.488226Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.0108, -0.1956, -0.0555, -0.1158,  0.1704,  0.1797, -0.0658, -0.0168,\n          0.1524, -0.1903],\n        [-0.1287, -0.2541, -0.0659,  0.0889,  0.2514,  0.0180, -0.0806, -0.0016,\n         -0.0307, -0.1419]], grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"class MLP(nn.Module):\n    # 用模型参数声明层。这里，我们声明两个全连接的层\n    def __init__(self):\n        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n        super().__init__()\n        self.hidden = nn.Linear(20, 256)  # 隐藏层\n        self.out = nn.Linear(256, 10)  # 输出层\n\n    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n    def forward(self, X):\n        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n        return self.out(F.relu(self.hidden(X)))","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:43:19.299501Z","iopub.execute_input":"2024-06-24T03:43:19.299873Z","iopub.status.idle":"2024-06-24T03:43:19.306846Z","shell.execute_reply.started":"2024-06-24T03:43:19.299844Z","shell.execute_reply":"2024-06-24T03:43:19.305639Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"net = MLP()\nnet(X)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:43:33.090552Z","iopub.execute_input":"2024-06-24T03:43:33.090944Z","iopub.status.idle":"2024-06-24T03:43:33.100273Z","shell.execute_reply.started":"2024-06-24T03:43:33.090913Z","shell.execute_reply":"2024-06-24T03:43:33.099136Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"tensor([[ 0.0727, -0.1494,  0.2290, -0.1414,  0.0866, -0.1176,  0.0610, -0.3061,\n          0.1342, -0.1805],\n        [ 0.0732, -0.1656,  0.2217, -0.0233, -0.0454, -0.0621,  0.0706, -0.2646,\n         -0.0679, -0.2397]], grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"class MySequential(nn.Module):\n    def __init__(self, *args):\n        super().__init__()\n        for idx, module in enumerate(args):\n            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n            # 变量_modules中。_module的类型是OrderedDict\n            self._modules[str(idx)] = module\n\n    def forward(self, X):\n        # OrderedDict保证了按照成员添加的顺序遍历它们\n        for block in self._modules.values():\n            X = block(X)\n        return X\nnet = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\nnet(X)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:51:59.216016Z","iopub.execute_input":"2024-06-24T03:51:59.217053Z","iopub.status.idle":"2024-06-24T03:51:59.228851Z","shell.execute_reply.started":"2024-06-24T03:51:59.217015Z","shell.execute_reply":"2024-06-24T03:51:59.227732Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"tensor([[-0.1698, -0.2842, -0.1841,  0.2658,  0.1992,  0.0830,  0.0945, -0.3605,\n         -0.4299,  0.0850],\n        [-0.2251, -0.1789, -0.2009,  0.2073,  0.1637,  0.0835,  0.0564, -0.2886,\n         -0.2907,  0.0412]], grad_fn=<AddmmBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"class FixedHiddenMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n        self.linear = nn.Linear(20, 20)\n\n    def forward(self, X):\n        X = self.linear(X)\n        # 使用创建的常量参数以及relu和mm函数\n        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n        # 复用全连接层。这相当于两个全连接层共享参数\n        X = self.linear(X)\n        # 控制流\n        while X.abs().sum() > 1:\n            X /= 2\n        return X.sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:59:38.023635Z","iopub.execute_input":"2024-06-24T03:59:38.024387Z","iopub.status.idle":"2024-06-24T03:59:38.031145Z","shell.execute_reply.started":"2024-06-24T03:59:38.024351Z","shell.execute_reply":"2024-06-24T03:59:38.030075Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"net = FixedHiddenMLP()\nnet(X)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T03:59:40.563809Z","iopub.execute_input":"2024-06-24T03:59:40.564156Z","iopub.status.idle":"2024-06-24T03:59:40.582154Z","shell.execute_reply.started":"2024-06-24T03:59:40.564130Z","shell.execute_reply":"2024-06-24T03:59:40.581243Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor(0.1820, grad_fn=<SumBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"class NestMLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n                                 nn.Linear(64, 32), nn.ReLU())\n        self.linear = nn.Linear(32, 16)\n\n    def forward(self, X):\n        return self.linear(self.net(X))\n\nchimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\nchimera(X)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T04:00:16.927638Z","iopub.execute_input":"2024-06-24T04:00:16.928349Z","iopub.status.idle":"2024-06-24T04:00:16.949062Z","shell.execute_reply.started":"2024-06-24T04:00:16.928256Z","shell.execute_reply":"2024-06-24T04:00:16.947612Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"tensor(0.0405, grad_fn=<SumBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"class ParallelBlock(nn.Module):\n    def __init__(self, net1, net2):\n        super().__init__()\n        self.net1 = net1\n        self.net2 = net2\n    \n    def forward(self, X):\n        return self.net2(self.net1(X))\n    \nnet = ParallelBlock(nn.Linear(16, 20), nn.Linear(20, 10))\nprint(net)\nfor param in net.parameters():\n    print(param)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T04:16:44.071105Z","iopub.execute_input":"2024-06-24T04:16:44.072218Z","iopub.status.idle":"2024-06-24T04:16:44.087914Z","shell.execute_reply.started":"2024-06-24T04:16:44.072177Z","shell.execute_reply":"2024-06-24T04:16:44.086560Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"ParallelBlock(\n  (net1): Linear(in_features=16, out_features=20, bias=True)\n  (net2): Linear(in_features=20, out_features=10, bias=True)\n)\nParameter containing:\ntensor([[ 0.0022, -0.2391, -0.1766, -0.1111,  0.1946, -0.1373,  0.1270,  0.1150,\n         -0.1431,  0.1772,  0.2208,  0.0518,  0.0910, -0.1587, -0.1977,  0.0252],\n        [-0.1535,  0.1074,  0.1740, -0.1748, -0.2230,  0.1172, -0.0341,  0.1435,\n          0.1740,  0.0219,  0.1573,  0.0502,  0.1006, -0.1087, -0.0902,  0.1015],\n        [-0.0294,  0.1433,  0.0591, -0.0903,  0.1870, -0.0871,  0.0945, -0.1308,\n          0.1308, -0.1036, -0.1887, -0.1883, -0.2167, -0.0962,  0.2048,  0.1057],\n        [-0.2183,  0.1912,  0.0793, -0.0108, -0.0114,  0.1152, -0.1019, -0.1016,\n         -0.0873, -0.0745, -0.0995,  0.0350, -0.2488,  0.0422,  0.1822,  0.0971],\n        [-0.1235, -0.1559,  0.0620, -0.1839,  0.2240, -0.2040,  0.1066, -0.1120,\n         -0.1873,  0.2370, -0.0728,  0.1199, -0.1927,  0.0527, -0.1473, -0.1268],\n        [ 0.0996,  0.0494,  0.1193,  0.0510, -0.0078,  0.0302,  0.0264,  0.1021,\n         -0.1429, -0.0967,  0.0060,  0.2057,  0.2268, -0.0389, -0.2219,  0.1233],\n        [-0.1842,  0.2362,  0.2396,  0.0117,  0.1434, -0.1481,  0.1161, -0.0953,\n         -0.0646, -0.2179,  0.1756, -0.2206, -0.1723, -0.0289, -0.1200, -0.1522],\n        [-0.1377,  0.1082,  0.0121,  0.0562, -0.0350,  0.0774, -0.1021, -0.0839,\n         -0.2091, -0.1252,  0.2148, -0.1172, -0.1617, -0.1224,  0.1262,  0.1060],\n        [-0.0969, -0.2239,  0.1228, -0.1720, -0.1376, -0.0401,  0.0786,  0.1859,\n          0.2479, -0.2460,  0.0954,  0.0388, -0.2299,  0.0782,  0.2400,  0.0236],\n        [-0.2230, -0.1605,  0.0701,  0.2098,  0.0609, -0.0362, -0.1045, -0.1364,\n         -0.0400,  0.1480,  0.2484,  0.0043,  0.0950, -0.2133,  0.2355, -0.1508],\n        [-0.2047,  0.0702, -0.1455,  0.0604,  0.2134, -0.0546, -0.0451, -0.1404,\n         -0.0867,  0.1479, -0.2145, -0.0369, -0.2367,  0.0793, -0.1656,  0.0008],\n        [ 0.0081, -0.0058,  0.1558, -0.0680, -0.1606,  0.0494, -0.1344, -0.0345,\n         -0.2268,  0.0464, -0.2397,  0.2358,  0.0648, -0.2260,  0.0744,  0.1287],\n        [ 0.2178,  0.1167, -0.0998,  0.1008,  0.2271, -0.0100,  0.1996,  0.1108,\n          0.1412,  0.0594,  0.2365,  0.1804,  0.1388, -0.0477,  0.0049,  0.2106],\n        [ 0.1739, -0.1819,  0.2426, -0.0129, -0.0853, -0.1480,  0.0877,  0.0181,\n          0.2355, -0.2159,  0.2177, -0.2415, -0.2253, -0.1130, -0.1583,  0.2005],\n        [ 0.0670, -0.2047, -0.2196, -0.1625, -0.1651,  0.1859,  0.1943,  0.2471,\n         -0.0925,  0.1670,  0.0890, -0.1998, -0.0382, -0.1009,  0.2041,  0.1893],\n        [ 0.1293,  0.0424, -0.0991,  0.0280,  0.2090,  0.1668, -0.1077,  0.0272,\n         -0.0004, -0.2199, -0.2206,  0.0437,  0.1715, -0.1266, -0.1196, -0.1498],\n        [ 0.0602,  0.1934,  0.2285,  0.2045,  0.1236, -0.0365,  0.0961, -0.0407,\n         -0.1378,  0.0696,  0.0722, -0.2324, -0.0536, -0.0864, -0.2187, -0.1412],\n        [-0.1498, -0.0480,  0.0439,  0.1244, -0.1906,  0.0077,  0.1383,  0.1723,\n         -0.1353, -0.0848, -0.2493, -0.1017, -0.1797, -0.1313, -0.1033, -0.0086],\n        [ 0.1990, -0.1810,  0.1499,  0.1629, -0.2374, -0.0071, -0.1756,  0.2175,\n          0.0121,  0.1956, -0.2239, -0.0041,  0.1661,  0.2266, -0.2132, -0.1880],\n        [ 0.2314,  0.0758,  0.1768, -0.0129, -0.1562,  0.1497,  0.1475, -0.1980,\n          0.1041,  0.0735,  0.0893,  0.1266, -0.1060,  0.1352,  0.0067,  0.0841]],\n       requires_grad=True)\nParameter containing:\ntensor([ 0.0506,  0.0325, -0.1533, -0.1598, -0.1598, -0.0026, -0.2431, -0.0839,\n         0.1494, -0.1298,  0.1013,  0.2175,  0.0018, -0.0379,  0.2395, -0.1662,\n         0.1383, -0.1585,  0.0174, -0.2029], requires_grad=True)\nParameter containing:\ntensor([[-0.1982,  0.1073, -0.1953,  0.0252, -0.1860,  0.1843, -0.0663,  0.1785,\n         -0.0776,  0.0220,  0.0658, -0.0229, -0.1023, -0.0233, -0.0368, -0.0082,\n          0.0465, -0.0916, -0.1274, -0.0307],\n        [ 0.1617,  0.1991, -0.1580, -0.0060,  0.1629, -0.1211, -0.0509,  0.0508,\n          0.0833,  0.2105,  0.0547, -0.1992, -0.1660,  0.1375, -0.2161, -0.1673,\n         -0.1274,  0.1161,  0.0778,  0.0726],\n        [ 0.1304,  0.0709, -0.0012, -0.0462, -0.0120, -0.2047, -0.1759, -0.0481,\n          0.1733,  0.0862,  0.0853,  0.0233,  0.2074,  0.1133,  0.0565, -0.1445,\n          0.2162,  0.1411,  0.0946,  0.0170],\n        [-0.2236, -0.0137,  0.0290,  0.1633,  0.0078, -0.1027,  0.2050,  0.0520,\n          0.1734, -0.0929,  0.0605,  0.2015,  0.1392, -0.2226,  0.0737, -0.0067,\n          0.1961,  0.0365,  0.1664, -0.0785],\n        [-0.0442,  0.0492, -0.1552,  0.1382, -0.1526, -0.0775,  0.0932,  0.0270,\n         -0.0798, -0.1760,  0.2118, -0.0985,  0.2068, -0.0958,  0.1587,  0.1596,\n          0.1498, -0.0369, -0.0164,  0.1953],\n        [ 0.1724,  0.1151, -0.0060, -0.1113, -0.0253, -0.2155,  0.1684, -0.0992,\n         -0.0825,  0.1757, -0.1267,  0.2113, -0.1392,  0.1279, -0.0137,  0.1743,\n         -0.0357, -0.1830, -0.1022, -0.0290],\n        [ 0.1983,  0.0852,  0.2013,  0.0087,  0.2187,  0.0050, -0.0843,  0.1639,\n         -0.0857,  0.1176,  0.0562,  0.0769, -0.1597, -0.1793, -0.2161,  0.1788,\n          0.1533,  0.1107, -0.2218, -0.0580],\n        [ 0.2111, -0.0607,  0.2050,  0.1598,  0.0642, -0.2010,  0.0439, -0.1075,\n          0.0283,  0.2212,  0.0098, -0.1283, -0.1891, -0.0632, -0.1508,  0.1522,\n         -0.1784, -0.0067,  0.2067,  0.0570],\n        [ 0.0939,  0.1550,  0.0394,  0.1898, -0.1377,  0.1837,  0.0245, -0.1639,\n          0.1000,  0.0255,  0.0435,  0.0475,  0.0501, -0.0473, -0.1898, -0.1090,\n         -0.1394,  0.1313, -0.0517, -0.0337],\n        [ 0.1456,  0.1221, -0.1263, -0.1506, -0.0762, -0.0805,  0.2100,  0.1289,\n         -0.1259, -0.1852,  0.1732,  0.1541,  0.0206, -0.1638, -0.1270, -0.0494,\n         -0.0261, -0.0988,  0.0669, -0.1882]], requires_grad=True)\nParameter containing:\ntensor([-0.1622, -0.0811,  0.0866, -0.1680, -0.0025,  0.0410, -0.2128, -0.0754,\n         0.0709, -0.0734], requires_grad=True)\n","output_type":"stream"}]}]}